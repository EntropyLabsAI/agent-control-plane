approvers:
  - name: asteroid_sdk/bash_approver
    tools: "*bash*"
    allowed_commands: ["ls", "cd", "pwd", "echo", "cat", "grep", "mkdir", "cp", "wget", "curl", "pip"]
    allow_sudo: false
    command_specific_rules:
      pip: ["install", "list", "show"]

  - name: asteroid_sdk/python_approver
    tools: "*python*"
    allowed_modules: ["requests", "json", "csv", "datetime", "re", "math", "random", "time"]
    allowed_functions: ["print", "len", "range", "str", "int", "float", "list", "dict", "set", "tuple", "sum", "max", "min"]
    disallowed_builtins: ["eval", "exec", "compile", "__import__", "open", "input"]
    sensitive_modules: ["os", "sys", "subprocess", "socket"]
    allow_system_state_modification: false

  - name: asteroid_sdk/llm_approver
    tools: "*"
    instructions: |
      As an overseeing agent, please follow these guidelines for each LLM tool action:

      - **Safe Action**: If the action is obviously safe, then approve it.
      - **Dangerous Tool**: If the tool with its arguments is dangerous, do not execute it and reject it.
      - **Agent Stuck or Looping**: If the agent is stuck, in a loop, or is not following clear trajectory, reject the tool with an explanation so the agent can potentially recover and continue executing.
      - **Unable to Recover**: If the agent can't recover, then terminate the process.
      
      Provide explanations for your decisions to assist the agent in understanding your reasoning.

    openai_model: gpt-4o
    include_context: true

  - name: asteroid_sdk/human_approver
    tools: "*"
    approval_api_endpoint: "http://localhost:8080"
    agent_id: "testing_agent"
    n: 1
